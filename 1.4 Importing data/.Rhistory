filter(pre_cid_4_pa$n_between_cid_3_4 > 5) %>%
filter(day_of_study_count > 10) %>%
filter(!is.na(delta_weight_cid1_to_cid4_percentage)) %>%filter(!is.na(slope_pa))
pre_cid_4_pa$ID = NULL
# Formatting the cid 2 names
colnames(pre_cid_2_pa) = colnames(pre_cid_3_pa)
names(pre_cid_2_pa)[names(pre_cid_2_pa) ==  "n_between_cid_2_3"] <-  "n_between_cid_1_2"
# CID 2 Data
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsSED_mean)) + geom_point() + geom_smooth()
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsLIGHT_mean)) + geom_point()+ geom_smooth()
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsMOD_mean)) + geom_point()+ geom_smooth()
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsVIG_mean)) + geom_point()+ geom_smooth()
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsMVPA_mean)) + geom_point()+ geom_smooth()
# CID2 model
cid2_m1 <- lmer(delta_weight_cid1_to_cid2_percentage ~ initial_wt_loss_percentage + HRRminsSED_mean *  HRRminsMVPA_mean + (1|participant_study_id), pre_cid_2_pa,
control = lmerControl(optimizer = "optimx",
calc.derivs = FALSE,
optCtrl = list(method = "nlminb",
starttests = FALSE,
kkt = FALSE)))
analyze(cid2_m1)
# CID 3 data
cid3_m1 <- lmer(delta_weight_cid1_to_cid2_percentage ~ initial_wt_loss_percentage + HRRminsSED_mean *  HRRminsMVPA_mean + (1|participant_study_id), pre_cid_3_pa,
control = lmerControl(optimizer = "optimx",
calc.derivs = FALSE,
optCtrl = list(method = "nlminb",
starttests = FALSE,
kkt = FALSE)))
analyze(cid3_m1)
# CID 3 data
cid4_m1 <- lmer(delta_weight_cid1_to_cid2_percentage ~ initial_wt_loss_percentage + HRRminsSED_mean  *  HRRminsMVPA_mean + (1|participant_study_id), pre_cid_4_pa,
control = lmerControl(optimizer = "optimx",
calc.derivs = FALSE,
optCtrl = list(method = "nlminb",
starttests = FALSE,
kkt = FALSE)))
analyze(cid4_m1)
# Bin by weight outcome
pre_cid_2_pa$w_outcome_cid_2<-cut(pre_cid_2_pa$delta_weight_cid1_to_cid2_percentage,
breaks = c(-Inf,
-3,
3,
Inf),
labels = c('loss',
'wlm',
'gain'))
# Bin by weight outcome
pre_cid_3_pa$w_outcome_cid_3<-cut(pre_cid_3_pa$delta_weight_cid1_to_cid3_percentage,
breaks = c(-Inf,
-3,
3,
Inf),
labels = c('loss',
'wlm',
'gain'))
# Bin by weight outcome
pre_cid_4_pa$w_outcome_cid_4<-cut(pre_cid_4_pa$delta_weight_cid1_to_cid4_percentage,
breaks = c(-Inf,
-3,
3,
Inf),
labels = c('loss',
'wlm',
'gain'))
#
pre_cid_2_pa%>%group_by(w_outcome_cid_2)%>%dplyr::summarise(count = n(),
sed = mean(HRRminsSED_mean),
mvpa = mean(HRRminsMVPA_mean))
pre_cid_3_pa%>%group_by(w_outcome_cid_3)%>%summarise(n = count(),
sed = mean(HRRminsSED_mean),
mvpa = mean(HRRminsMVPA_mean))
pre_cid_4_pa%>%group_by(w_outcome_cid_4)%>%summarise(n = count(),
sed = mean(HRRminsSED_mean),
mvpa = mean(HRRminsMVPA_mean))
# WHO levels
#The WHO guidelines recommend at least 150 min of moderate-intensity
#aerobic physical activity or 75 min of vigorous intensity physical activity per week. For additional benefits such as the maintenance of body
#weight, the amount of recommended physical activity is doubled [39]. (foright )
# 200 MINS ACCORDING TO WLM
pre_cid_4_pa$ACSM_300<- ifelse(pre_cid_4_pa$HRRminsMVPA_mean > ceiling(300/7), TRUE, FALSE)
pre_cid_4_pa%>%
group_by(ACSM_300) %>% summarise(count = n(),
mean_wt = mean(delta_weight_cid1_to_cid4_percentage))
pre_cid_4_pa$ACSM_150<- ifelse(pre_cid_4_pa$HRRminsVIG_mean > ceiling(150/7), TRUE, FALSE)
pre_cid_4_pa%>%
group_by(ACSM_150) %>% summarise(count = n(),
mean_wt = mean(delta_weight_cid1_to_cid4_percentage))
# Anovas
library(car)
options(contrasts = c("contr.sum", "contr.poly"))
aov_4_mvpa<-Anova(lm(HRRminsMVPA_mean ~ w_outcome_cid_4, data=pre_cid_4_pa), type=3)
tukey_4_mvpa=TukeyHSD(aov(HRRminsMVPA_mean ~ w_outcome_cid_4, data=pre_cid_4_pa))
summary(aov_4_mvpa)
aov_4_sb<-Anova(lm(HRRminsSED_mean ~ w_outcome_cid_4, data=pre_cid_4_pa), type=3)
tukey_4_sb=TukeyHSD(aov(HRRminsSED_mean ~ w_outcome_cid_4, data=pre_cid_4_pa))
mean(pre_cid_2_pa$delta_weight_cid1_to_cid2_percentage, na.rm = T)
sd(pre_cid_2_pa$delta_weight_cid1_to_cid2_percentage, na.rm = T)
mean(pre_cid_3_pa$delta_weight_cid1_to_cid3_percentage, na.rm = T)
sd(pre_cid_3_pa$delta_weight_cid1_to_cid3_percentage, na.rm = T)
mean(pre_cid_4_pa$delta_weight_cid1_to_cid4_percentage, na.rm = T)
sd(pre_cid_4_pa$delta_weight_cid1_to_cid4_percentage, na.rm = T)
range(pre_cid_4_pa$delta_weight_cid1_to_cid4_percentage, na.rm = T)
range(pre_cid_4_pa$HRRminsMVPA_mean, na.rm = T)
pre_cid_4_pa%>%group_by(w_outcome_cid_4)%>%summarise(count = n())
# Is there an effect of direction of pa
pre_cid_4_pa$slope_pa_bool = ifelse(pre_cid_4_pa$slope_pa >= 0, 'pos', 'neg')
pre_cid_4_pa$slope_sb_bool = ifelse(pre_cid_4_pa$slope_sb >= 0, 'pos', 'neg')
slope_aov<-Anova(lm(delta_weight_cid1_to_cid4_percentage ~ slope_pa_bool, data=pre_cid_4_pa), type=3)
tukey_slope=TukeyHSD(aov(delta_weight_cid1_to_cid4_percentage ~ slope_pa_bool, data=pre_cid_4_pa))
colSums(is.na(pre_cid_4_pa))
slope_aov
tukey_slope
nrow(pre_cid_4_pa)
pre_cid_4_pa%>%group_by(w_outcome_cid_4)%>%summarise(n = count(),
sed = mean(HRRminsSED_mean),
mvpa = mean(HRRminsMVPA_mean))
pre_cid_4_pa%>%group_by(w_outcome_cid_4)%>%summarise(n = count(),
sed = mean(HRRminsSED_mean),
mvpa = mean(HRRminsMVPA_mean))
# libraries
library(psycho)
library(lme4)
library(lmerTest)
library(dplyr)
library(ggplot2)
library(optimx)
library(nlme)
# Paper of interest
## https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4427898/pdf/nihms684891.pdf
# PA wlm
pre_cid_2_pa = read.csv('E:/Google Drive/PhD/Thesis/PA WLM PAPER/Paper/Data/pre_cid_2_pa.csv')
pre_cid_3_pa = read.csv('E:/Google Drive/PhD/Thesis/PA WLM PAPER/Paper/Data/pre_cid_3_pa.csv')
pre_cid_4_pa = read.csv('E:/Google Drive/PhD/Thesis/PA WLM PAPER/Paper/Data/pre_cid_4_pa.csv')
# availability of 5 days
pre_cid_2_pa = pre_cid_2_pa[as.character(pre_cid_2_pa$ecid2_cid2_date)!="",]
pre_cid_2_pa = pre_cid_2_pa %>%
filter(pre_cid_2_pa$X..n_between_cid_1_2...... > 5) %>%
filter(!is.na(delta_weight_cid1_to_cid2_percentage))
# availability of 5 days
pre_cid_3_pa = pre_cid_3_pa[as.character(pre_cid_3_pa$ecid3_cid3_date)!="",]
pre_cid_3_pa = pre_cid_3_pa %>%
filter(pre_cid_3_pa$n_between_cid_2_3 > 5)%>%
filter(day_of_study_count > 10) %>%
filter(!is.na(delta_weight_cid1_to_cid3_percentage))
pre_cid_3_pa$ID = NULL
# availability of 5 days
pre_cid_4_pa = pre_cid_4_pa[as.character(pre_cid_4_pa$ecid4_cid4_date)!="",]
pre_cid_4_pa = pre_cid_4_pa %>%
filter(pre_cid_4_pa$n_between_cid_3_4 > 5) %>%
filter(day_of_study_count > 10) %>%
filter(!is.na(delta_weight_cid1_to_cid4_percentage)) %>%filter(!is.na(slope_pa))
pre_cid_4_pa$ID = NULL
# Formatting the cid 2 names
colnames(pre_cid_2_pa) = colnames(pre_cid_3_pa)
names(pre_cid_2_pa)[names(pre_cid_2_pa) ==  "n_between_cid_2_3"] <-  "n_between_cid_1_2"
# CID 2 Data
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsSED_mean)) + geom_point() + geom_smooth()
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsLIGHT_mean)) + geom_point()+ geom_smooth()
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsMOD_mean)) + geom_point()+ geom_smooth()
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsVIG_mean)) + geom_point()+ geom_smooth()
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsMVPA_mean)) + geom_point()+ geom_smooth()
# CID2 model
cid2_m1 <- lmer(delta_weight_cid1_to_cid2_percentage ~ initial_wt_loss_percentage + HRRminsSED_mean *  HRRminsMVPA_mean + (1|participant_study_id), pre_cid_2_pa,
control = lmerControl(optimizer = "optimx",
calc.derivs = FALSE,
optCtrl = list(method = "nlminb",
starttests = FALSE,
kkt = FALSE)))
analyze(cid2_m1)
# CID 3 data
cid3_m1 <- lmer(delta_weight_cid1_to_cid2_percentage ~ initial_wt_loss_percentage + HRRminsSED_mean *  HRRminsMVPA_mean + (1|participant_study_id), pre_cid_3_pa,
control = lmerControl(optimizer = "optimx",
calc.derivs = FALSE,
optCtrl = list(method = "nlminb",
starttests = FALSE,
kkt = FALSE)))
analyze(cid3_m1)
# CID 3 data
cid4_m1 <- lmer(delta_weight_cid1_to_cid2_percentage ~ initial_wt_loss_percentage + HRRminsSED_mean  *  HRRminsMVPA_mean + (1|participant_study_id), pre_cid_4_pa,
control = lmerControl(optimizer = "optimx",
calc.derivs = FALSE,
optCtrl = list(method = "nlminb",
starttests = FALSE,
kkt = FALSE)))
analyze(cid4_m1)
summary(cid4_m1)
# Bin by weight outcome
pre_cid_4_pa$w_outcome_cid_4<-cut(pre_cid_4_pa$delta_weight_cid1_to_cid4_percentage,
breaks = c(-Inf,
-3,
3,
Inf),
labels = c('loss',
'wlm',
'gain'))
pre_cid_4_pa%>%group_by(w_outcome_cid_4)%>%summarise(n = count(),
sed = mean(HRRminsSED_mean),
mvpa = mean(HRRminsMVPA_mean))
pre_cid_4_pa%>%dplyr::group_by(w_outcome_cid_4)%>%dplyr::summarise(n = count(),
sed = mean(HRRminsSED_mean),
mvpa = mean(HRRminsMVPA_mean))
# libraries
library(psycho)
library(lme4)
library(lmerTest)
library(dplyr)
library(ggplot2)
library(optimx)
library(nlme)
# Paper of interest
## https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4427898/pdf/nihms684891.pdf
# PA wlm
pre_cid_2_pa = read.csv('E:/Google Drive/PhD/Thesis/PA WLM PAPER/Paper/Data/pre_cid_2_pa.csv')
pre_cid_3_pa = read.csv('E:/Google Drive/PhD/Thesis/PA WLM PAPER/Paper/Data/pre_cid_3_pa.csv')
pre_cid_4_pa = read.csv('E:/Google Drive/PhD/Thesis/PA WLM PAPER/Paper/Data/pre_cid_4_pa.csv')
# availability of 5 days
pre_cid_2_pa = pre_cid_2_pa[as.character(pre_cid_2_pa$ecid2_cid2_date)!="",]
pre_cid_2_pa = pre_cid_2_pa %>%
filter(pre_cid_2_pa$X..n_between_cid_1_2...... > 5) %>%
filter(!is.na(delta_weight_cid1_to_cid2_percentage))
# availability of 5 days
pre_cid_3_pa = pre_cid_3_pa[as.character(pre_cid_3_pa$ecid3_cid3_date)!="",]
pre_cid_3_pa = pre_cid_3_pa %>%
filter(pre_cid_3_pa$n_between_cid_2_3 > 5)%>%
filter(day_of_study_count > 10) %>%
filter(!is.na(delta_weight_cid1_to_cid3_percentage))
pre_cid_3_pa$ID = NULL
# availability of 5 days
pre_cid_4_pa = pre_cid_4_pa[as.character(pre_cid_4_pa$ecid4_cid4_date)!="",]
pre_cid_4_pa = pre_cid_4_pa %>%
filter(pre_cid_4_pa$n_between_cid_3_4 > 5) %>%
filter(day_of_study_count > 10) %>%
filter(!is.na(delta_weight_cid1_to_cid4_percentage)) %>%filter(!is.na(slope_pa))
pre_cid_4_pa$ID = NULL
# Formatting the cid 2 names
colnames(pre_cid_2_pa) = colnames(pre_cid_3_pa)
names(pre_cid_2_pa)[names(pre_cid_2_pa) ==  "n_between_cid_2_3"] <-  "n_between_cid_1_2"
# CID 2 Data
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsSED_mean)) + geom_point() + geom_smooth()
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsLIGHT_mean)) + geom_point()+ geom_smooth()
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsMOD_mean)) + geom_point()+ geom_smooth()
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsVIG_mean)) + geom_point()+ geom_smooth()
ggplot(pre_cid_2_pa, aes(x = delta_weight_cid1_to_cid2_percentage, y = HRRminsMVPA_mean)) + geom_point()+ geom_smooth()
# CID2 model
cid2_m1 <- lmer(delta_weight_cid1_to_cid2_percentage ~ initial_wt_loss_percentage + HRRminsSED_mean *  HRRminsMVPA_mean + (1|participant_study_id), pre_cid_2_pa,
control = lmerControl(optimizer = "optimx",
calc.derivs = FALSE,
optCtrl = list(method = "nlminb",
starttests = FALSE,
kkt = FALSE)))
analyze(cid2_m1)
# CID 3 data
cid3_m1 <- lmer(delta_weight_cid1_to_cid2_percentage ~ initial_wt_loss_percentage + HRRminsSED_mean *  HRRminsMVPA_mean + (1|participant_study_id), pre_cid_3_pa,
control = lmerControl(optimizer = "optimx",
calc.derivs = FALSE,
optCtrl = list(method = "nlminb",
starttests = FALSE,
kkt = FALSE)))
analyze(cid3_m1)
# CID 3 data
cid4_m1 <- lmer(delta_weight_cid1_to_cid2_percentage ~ initial_wt_loss_percentage + HRRminsSED_mean  *  HRRminsMVPA_mean + (1|participant_study_id), pre_cid_4_pa,
control = lmerControl(optimizer = "optimx",
calc.derivs = FALSE,
optCtrl = list(method = "nlminb",
starttests = FALSE,
kkt = FALSE)))
analyze(cid4_m1)
# Bin by weight outcome
pre_cid_2_pa$w_outcome_cid_2<-cut(pre_cid_2_pa$delta_weight_cid1_to_cid2_percentage,
breaks = c(-Inf,
-3,
3,
Inf),
labels = c('loss',
'wlm',
'gain'))
# Bin by weight outcome
pre_cid_3_pa$w_outcome_cid_3<-cut(pre_cid_3_pa$delta_weight_cid1_to_cid3_percentage,
breaks = c(-Inf,
-3,
3,
Inf),
labels = c('loss',
'wlm',
'gain'))
# Bin by weight outcome
pre_cid_4_pa$w_outcome_cid_4<-cut(pre_cid_4_pa$delta_weight_cid1_to_cid4_percentage,
breaks = c(-Inf,
-3,
3,
Inf),
labels = c('loss',
'wlm',
'gain'))
#
pre_cid_2_pa%>%group_by(w_outcome_cid_2)%>%dplyr::summarise(count = n(),
sed = mean(HRRminsSED_mean),
mvpa = mean(HRRminsMVPA_mean))
pre_cid_3_pa%>%group_by(w_outcome_cid_3)%>%summarise(n = count(),
sed = mean(HRRminsSED_mean),
mvpa = mean(HRRminsMVPA_mean))
pre_cid_4_pa%>%dplyr::group_by(w_outcome_cid_4)%>%dplyr::summarise(n = count(),
sed = mean(HRRminsSED_mean),
mvpa = mean(HRRminsMVPA_mean))
pre_cid_4_pa%>%group_by(w_outcome_cid_4)%>%summarise(count = n())
pre_cid_4_pa%>%dplyr::group_by(w_outcome_cid_4)%>%dplyr::summarise(nn = n(),
sed = mean(HRRminsSED_mean),
mvpa = mean(HRRminsMVPA_mean))
mean(pre_cid_4_pa$delta_weight_cid1_to_cid4_percentage, na.rm = T)
sd(pre_cid_4_pa$delta_weight_cid1_to_cid4_percentage, na.rm = T)
range(pre_cid_4_pa$delta_weight_cid1_to_cid4_percentage, na.rm = T)
pre_cid_4_pa$slope_pa_bool = ifelse(pre_cid_4_pa$slope_pa >= 0, 'pos', 'neg')
pre_cid_4_pa$slope_sb_bool = ifelse(pre_cid_4_pa$slope_sb >= 0, 'pos', 'neg')
slope_aov<-Anova(lm(delta_weight_cid1_to_cid4_percentage ~ slope_pa_bool, data=pre_cid_4_pa), type=3)
tukey_slope=TukeyHSD(aov(delta_weight_cid1_to_cid4_percentage ~ slope_pa_bool, data=pre_cid_4_pa))
tukey_slope
pre_cid_4_pa%>%group_by(w_outcome_cid_4)%>%summarise(count = n())
pre_cid_4_pa%>%group_by(slope_pa_bool)%>%summarise(count = n(),
d_wt = mean(delta_weight_cid1_to_cid4_percentage),
d_wt_sd = mean(delta_weight_cid1_to_cid4_percentage))
pre_cid_4_pa%>%group_by(slope_pa_bool)%>%summarise(count = n(),
d_wt = mean(delta_weight_cid1_to_cid4_percentage),
d_wt_sd = sd(delta_weight_cid1_to_cid4_percentage))
summary(slope_aov)
slope_aov<-Anova(lm(delta_weight_cid1_to_cid4_percentage ~ slope_pa_bool, data=pre_cid_4_pa), type=3)
# Anovas
library(car)
slope_aov<-Anova(lm(delta_weight_cid1_to_cid4_percentage ~ slope_pa_bool, data=pre_cid_4_pa), type=3)
summary(slope_aov)
slope_aov
summary(cid4_m1)
print(1);100
print(1:100)
#JP diva pa data
#relevant library
library('Rmisc')
library('dplyr')
library('zoo')
library('data.table')
library('reshape')
library('ggplot2')
library('car')
library('agricolae')
library('stats')
library('multcomp')
library('lsmeans')
library('gridExtra')
library('afex')
library('ggpubr')
# Hourly
# hourly analysis
files <- list.files("E:/Google Drive/PhD/Thesis/Protocol and descriptive paper/Descriptive paper/Data/hrrhourlydata", pattern = "*.csv", full.names = T)
# files <- list.files("/Users/Ruairi/Google Drive/PhD/Thesis/Protocol and descriptive paper/Descriptive paper/Data/hrrhourlydata", pattern = "*.csv", full.names = T)
hourly_pa <- lapply(files, fread)
hourly_pa <- bind_rows(hourly_pa)
hourly_pa<-merge(descriptives,
hourly_pa,
by.x = "iplist",
by.y = "participant_id",
all =T)
hourly_pa<-hourly_pa%>%
filter(!is.na(hour))%>%
filter(study_day < 90)
hourly_pa$HRRminsMVPA = hourly_pa$HRRminsMOD + hourly_pa$HRRminsVIG
str(hourly_pa)
dim(hourly_pa)
library(car)
?aov_ez
library(Anova)
library(afex)
?aov_ez
?aov_ez
data_1 = data.frame(id = c(1,2,3,4,5),
age = c(10, 20, 30, 40, 50),
sex = c('M', 'F', 'M', 'F', 'M'))
View(data_1)
data_2 = data.frame(id = c(6,7,8,9,10),
age = c(15, 25, 35, 45, 55),
sex = c('M', 'F', 'M', 'F', 'M'))
View(data_2)
# Bind the two data frames to create a new dataframe (data_3)
data_3 = rbind(data_1, data_2)
data_3$mod = data_3$age %% 2
View(data_3)
# Create 4 new data frames: Even aged men/women, odd aged men/women
ev_m = data_3[data_3$sex == 'M' & data_3$mod == 0, ]
# Create 4 new data frames: Even aged men/women, odd aged men/women
ev_m = data_3[data_3$sex == 'M' & data_3$mod == 0, ]
odd_m = data_3[data_3$sex == 'M' & data_3$mod == 1, ]
ev_f = data_3[data_3$sex == 'F' & data_3$mod == 0, ]
odd_f = data_3[data_3$sex == 'F' & data_3$mod == 1, ]
getwd()
setwd('E:/Google Drive')
getwd()
getwd()
setwd("E:/Google Drive/R Seminar (PR)/Introduction-to-R/1.4 Importing data")
read.table('text_file.txt')
txt=read.table('text_file.txt')
help("read.table")
txt_false=read.table('text_file.txt', header = FALSE)
txt_true=read.table('text_file.txt', header =TRUE)
View(txt_true)
txt_false=read.table('text_file.txt', header = FALSE)
txt_true=read.table('text_file.txt', header =TRUE)
View(txt_true)
View(txt_false)
View(txt_true)
txt_true=read.table('text_file.txt', stringsAsFactors = TRUE)
txt_true=read.table('text_file.txt', stringsAsFactors = FALSE)
txt_false=read.table('text_file.txt', stringsAsFactors = FALSE)
txt_true=read.table('text_file.txt', stringsAsFactors = TRUE)
txt_false=read.table('text_file.txt', stringsAsFactors = FALSE)
happiness_2015=read.xlsx("happiness_report.xlsx", sheetName = "2015")
library(xlsx)
library('xlsx')
install.packages('xlsx')
library('xlsx')
install.packages("readxl")
library('readxl')
happiness_2015=read.excel("happiness_report.xlsx", sheet = "2015")
happiness_2015=read_excel("happiness_report.xlsx", sheet = "2015")
happiness_2015=read_excel("happiness_report.xlsx", sheet = "2015")
happiness_2016=read_excel("happiness_report.xlsx", sheet = "2016")
happiness_2015=read_excel("happiness_report.xlsx", sheet = "2015")
happiness_2016=read_excel("happiness_report.xlsx", sheet = "2016")
happiness_2017=read_excel("happiness_report.xlsx", sheet = "2017")
happiness_2018=read_excel("happiness_report.xlsx", sheet = "2018")
happiness_2019=read_excel("happiness_report.xlsx", sheet = "2019")
#2. Select the columns representing rank and country name from each
happiness_2015=happiness_2015[1:2, ]
View(happiness_2015)
#1. Read in each of the happiness files
happiness_2015=read_excel("happiness_report.xlsx", sheet = "2015")
happiness_2016=read_excel("happiness_report.xlsx", sheet = "2016")
happiness_2017=read_excel("happiness_report.xlsx", sheet = "2017")
happiness_2018=read_excel("happiness_report.xlsx", sheet = "2018")
happiness_2019=read_excel("happiness_report.xlsx", sheet = "2019")
#2. Select the columns representing rank and country name from each
happiness_2015=happiness_2015[, 1:2]
#2. Select the columns representing rank and country name from each
happiness_2015=happiness_2015[, c(1,3)]
#1. Read in each of the happiness files
happiness_2015=read_excel("happiness_report.xlsx", sheet = "2015")
happiness_2016=read_excel("happiness_report.xlsx", sheet = "2016")
happiness_2017=read_excel("happiness_report.xlsx", sheet = "2017")
happiness_2018=read_excel("happiness_report.xlsx", sheet = "2018")
happiness_2019=read_excel("happiness_report.xlsx", sheet = "2019")
#2. Select the columns representing rank and country name from each
happiness_2015=happiness_2015[, c(1,3)]
happiness_2016=happiness_2016[, c(1,3)]
happiness_2017=happiness_2017[, c(1,3)]
happiness_2018=happiness_2018[, c(1,2)]
happiness_2019=happiness_2018[, c(2,1)]
#1. Read in each of the happiness files
happiness_2015=read_excel("happiness_report.xlsx", sheet = "2015")
happiness_2016=read_excel("happiness_report.xlsx", sheet = "2016")
happiness_2017=read_excel("happiness_report.xlsx", sheet = "2017")
happiness_2018=read_excel("happiness_report.xlsx", sheet = "2018")
happiness_2019=read_excel("happiness_report.xlsx", sheet = "2019")
#2. Select the columns representing rank and country name from each
happiness_2015=happiness_2015[, c(1,3)]
happiness_2016=happiness_2016[, c(1,3)]
happiness_2017=happiness_2017[, c(1,3)]
happiness_2018=happiness_2018[, c(2,1)] # note 2 first for order
happiness_2019=happiness_2018[, c(2,1)]
#1. Read in each of the happiness files
happiness_2015=read_excel("happiness_report.xlsx", sheet = "2015")
happiness_2016=read_excel("happiness_report.xlsx", sheet = "2016")
happiness_2017=read_excel("happiness_report.xlsx", sheet = "2017")
happiness_2018=read_excel("happiness_report.xlsx", sheet = "2018")
happiness_2019=read_excel("happiness_report.xlsx", sheet = "2019")
happiness_2018=happiness_2018[, c(2,1)] # note 2 first for order
happiness_2019=happiness_2018[, c(2,1)]
#1. Read in each of the happiness files
happiness_2015=read_excel("happiness_report.xlsx", sheet = "2015")
happiness_2016=read_excel("happiness_report.xlsx", sheet = "2016")
happiness_2017=read_excel("happiness_report.xlsx", sheet = "2017")
happiness_2018=read_excel("happiness_report.xlsx", sheet = "2018")
happiness_2019=read_excel("happiness_report.xlsx", sheet = "2019")
#2. Select the columns representing rank and country name from each
happiness_2015=happiness_2015[, c(1,3)]
happiness_2016=happiness_2016[, c(1,3)]
happiness_2017=happiness_2017[, c(1,3)]
happiness_2018=happiness_2018[, c(2,1)] # note 2 first for order
happiness_2019=happiness_2019[, c(2,1)]
#3. Add a label column to each file detailing the year
happiness_2015$year='2015'
happiness_2016$year='2016'
happiness_2017$year='2017'
happiness_2018$year='2018'
happiness_2019$year='2019'
#4. Join the files together by rows (hint: check the column names!)
# rename first
names = c('Country', 'Rank', 'Year')
colnames(happiness_2015) = names
colnames(happiness_2016) = names
colnames(happiness_2017) = names
colnames(happiness_2018) = names
colnames(happiness_2019) = names
#4. Join the files together by rows (hint: check the column names!)
# rename first
names = c('Country', 'Rank', 'Year')
colnames(happiness_2015) = names
colnames(happiness_2016) = names
colnames(happiness_2017) = names
colnames(happiness_2018) = names
colnames(happiness_2019) = names
hap = rbind(happiness_2015, happiness_2016)
hap = rbind(happiness_2015, happiness_2016)
hap = rbind(hap, happiness_2017)
hap = rbind(hap, happiness_2018)
hap = rbind(hap, happiness_2019)
